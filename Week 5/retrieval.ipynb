{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "from data_utils import *\n",
    "import fasttext, re, json, faiss\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from model import Net\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfe375bf1384c5989022581bbc1de3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/92.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luis\\miniconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Luis\\.cache\\huggingface\\hub\\models--mistralai--Mixtral-8x7B-Instruct-v0.1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fbe04bd2514860b58c85b294f799bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513431cce3624a1baf275000d2d99683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00019.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d56502f4be4e749ba451820129fdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde888010d124c6ab0098dc0f314bbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00019.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=True, device_map=\"auto\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'IMG_WIDTH': 224,\n",
    "    'IMG_HEIGHT': 224,\n",
    "    'TEST_DATASET_DIR': '../Week 4/data/val2014',\n",
    "    'batch_size': 16,\n",
    "    'n_neighbors': 5,\n",
    "    'classifier': 'knn',\n",
    "    'text_encoder_type': 'bert',\n",
    "    'mode': \"img2text\",\n",
    "    'metric': 'euclidean',\n",
    "    'voronoi_cells': 64,\n",
    "    'lookup_cells': 8,\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File containing images info (file_name)\n",
    "with open(f\"data/captions_val2014.json\", 'r') as f:\n",
    "    captions_val = json.load(f)\n",
    "\n",
    "# Dataset for the retrieval\n",
    "retrieval_dataset = CocoMetricDataset(\n",
    "    root=config[\"TEST_DATASET_DIR\"],\n",
    "    captions_file=captions_val,\n",
    "    transforms=CustomTransform(config, mode=\"train\"))\n",
    "\n",
    "dataloader = DataLoader(retrieval_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=coco_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(config['text_encoder_type'])\n",
    "model.load_state_dict(torch.load(\"pretrained/image2text_hard_ft.pth\", map_location=config['device']))\n",
    "model = model.to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(loader, model, device, mode=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        db_features, query_features, labels = [], [], []\n",
    "        for img, captions, label in tqdm(loader, total=len(loader), desc='Extracting features'):\n",
    "            img_f, text_f = model(img.to(device), captions)\n",
    "            if mode == \"img2text\":\n",
    "                db_features.append(text_f.cpu().numpy())\n",
    "                query_features.append(img_f.cpu().numpy())\n",
    "            else:\n",
    "                db_features.append(img_f.cpu().numpy())\n",
    "                query_features.append(text_f.cpu().numpy())\n",
    "\n",
    "            labels.append(label)\n",
    "        return np.concatenate(db_features).astype('float32'), np.concatenate(query_features).astype('float32'), np.concatenate(labels)\n",
    "\n",
    "# k-NN Classifier for Image Retrieval\n",
    "class ImageRetrievalSystem:\n",
    "    def __init__(self, model, database_loader, config):\n",
    "        self.model = model\n",
    "        self.database_loader = database_loader\n",
    "        self.device = config['device']\n",
    "        self.mode = config['mode']\n",
    "\n",
    "        self.dim = 2048\n",
    "        self.classifier_type = config['classifier']\n",
    "        self.n_neighbors = config['n_neighbors']\n",
    "\n",
    "        if self.classifier_type == 'knn':\n",
    "            self.classifier = NearestNeighbors(n_neighbors=config['n_neighbors'], metric=config['metric'])\n",
    "        else:\n",
    "            self.classifier = faiss.IndexIVFFlat(faiss.IndexFlatL2(self.dim), self.dim, config['voronoi_cells'])\n",
    "            self.classifier.nprobe = config['lookup_cells']\n",
    "\n",
    "    def fit_and_retrieve(self):\n",
    "\n",
    "        db_features, query_features, labels = extract_features(self.database_loader, self.model, self.device, mode=self.mode)\n",
    "        \n",
    "        if self.classifier_type == 'knn':\n",
    "            self.classifier.fit(db_features)\n",
    "            _, predictions = self.classifier.kneighbors(query_features, return_distance=True)\n",
    "        else:\n",
    "            self.classifier.train(db_features)\n",
    "            self.classifier.add(db_features)\n",
    "            _, predictions = self.classifier.search(query_features, self.n_neighbors)\n",
    "\n",
    "        return predictions, labels   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(preds, true_label):\n",
    "    \"\"\"Calculate average precision for a single class/query.\"\"\"\n",
    "    relevant = np.cumsum([1 if p == true_label else 0 for p in preds])\n",
    "    precision_at_k = relevant / np.arange(1, len(preds) + 1)\n",
    "    return np.sum(precision_at_k * (relevant > 0)) / np.maximum(np.sum(relevant > 0), 1)\n",
    "\n",
    "def evaluate(predictions, labels):\n",
    "    # Prec@1\n",
    "    prec_at_1 = np.mean([1 if predictions[i, 0] == labels[i] else 0 for i in range(predictions.shape[0])])\n",
    "\n",
    "    # Prec@5\n",
    "    prec_at_5 = np.mean([np.any([1 if predictions[i, j] == labels[i] else 0 for j in range(5)]) for i in range(predictions.shape[0])])\n",
    "\n",
    "    ap_scores = [average_precision(predictions[i], labels[i]) for i in range(predictions.shape[0])]\n",
    "    mean_ap = np.mean(ap_scores)\n",
    "\n",
    "    return prec_at_1, prec_at_5, mean_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ImageRetrievalSystem(model, dataloader, config)\n",
    "predictions_idx, labels = pipeline.fit_and_retrieve()\n",
    "\n",
    "predictions = labels[predictions_idx]\n",
    "\n",
    "prec_at_1, prec_at_5, mean_ap = evaluate(predictions, labels)\n",
    "\n",
    "print(f'Precision at 1: {prec_at_1:.4f}')\n",
    "print(f'Precision at 5: {prec_at_5:.4f}')\n",
    "print(f'Mean Average Precision: {mean_ap:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to denormalize image\n",
    "def denormalize(normalized_img):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    denormalized_img = normalized_img * std[:, None, None] + mean[:, None, None]\n",
    "    return denormalized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "retrieval_vis_data = [retrieval_dataset[x] for x in np.unique(predictions_idx[:n].flatten())]\n",
    "gt_vis_data = [retrieval_dataset[x] for x in range(n)]\n",
    "\n",
    "retrieved_embeddings = []\n",
    "for img, text, _ in retrieval_vis_data:\n",
    "    with torch.no_grad():\n",
    "        img_f, text_f = model(img.unsqueeze(0).to(config['device']), [text])\n",
    "    if config['mode'] == \"img2text\":\n",
    "        retrieved_embeddings.append(text_f.cpu().numpy())\n",
    "    else:\n",
    "        retrieved_embeddings.append(img_f.cpu().numpy())\n",
    "\n",
    "gt_embeddings, query_embeddings = [], []\n",
    "for img, text, _ in gt_vis_data:\n",
    "    with torch.no_grad():\n",
    "        img_f, text_f = model(img.unsqueeze(0).to(config['device']), [text])\n",
    "    if config['mode'] == \"img2text\":\n",
    "        gt_embeddings.append(text_f.cpu().numpy())\n",
    "        query_embeddings.append(img_f.cpu().numpy())\n",
    "    else:\n",
    "        gt_embeddings.append(img_f.cpu().numpy())\n",
    "        query_embeddings.append(text_f.cpu().numpy())\n",
    "\n",
    "retrieved_embeddings = np.concatenate(retrieved_embeddings).astype('float32')\n",
    "query_embeddings = np.concatenate(query_embeddings).astype('float32')\n",
    "gt_embeddings = np.concatenate(gt_embeddings).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox, TextArea\n",
    "import textwrap\n",
    "from sklearn.manifold import TSNE\n",
    "from imojify import imojify\n",
    "\n",
    "textlength = 50\n",
    "\n",
    "# Combine both sets of embeddings for UMAP\n",
    "combined_embeddings = np.vstack([query_embeddings, retrieved_embeddings, gt_embeddings]).reshape(-1, pipeline.dim)\n",
    "\n",
    "# Perform t-SNE on the combined embeddings\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto', perplexity=1.5, metric=config['metric'], random_state=123).fit_transform(combined_embeddings)\n",
    "\n",
    "# Split the transformed embeddings back into queries and images\n",
    "queries_2d, retrieved_2d, gt_2d = X_embedded[:len(query_embeddings)], X_embedded[len(query_embeddings):len(query_embeddings)+len(retrieved_embeddings)], X_embedded[len(query_embeddings)+len(retrieved_embeddings):]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "radius = 5000 if config['mode'] == \"img2text\" else 15000\n",
    "\n",
    "# Plot retrieved\n",
    "for i, (x, y) in enumerate(retrieved_2d):\n",
    "    if config['mode'] == \"img2text\": # retrieve text\n",
    "        text = textwrap.fill(retrieval_vis_data[i][1][:textlength]+'...', width=20)\n",
    "        text_box = TextArea(text, textprops=dict(color='black', size=7, ha='center', va='bottom'))\n",
    "        ab = AnnotationBbox(text_box, (x, y), frameon=True, bboxprops=dict(boxstyle='round,pad=0.5', facecolor='white', edgecolor='black', alpha=0.7))\n",
    "        ax.add_artist(ab)\n",
    "    else:\n",
    "        img = denormalize(retrieval_vis_data[i][0]).permute(1, 2, 0)\n",
    "        imgbox = OffsetImage(img, zoom=0.35)\n",
    "        ab = AnnotationBbox(imgbox, (x, y), frameon=True)\n",
    "        ax.add_artist(ab)\n",
    "    # plot a red circle around the retrieved with legend\n",
    "    if i == 0:\n",
    "        ax.scatter(x, y, s=100, facecolors='none', edgecolors='red', label='Retrieved')\n",
    "    ax.scatter(x, y, s=radius, facecolors='none', edgecolors='red')\n",
    "\n",
    "# Plot gt\n",
    "for i, (x, y) in enumerate(gt_2d):\n",
    "    if config['mode'] == \"img2text\": # retrieve text\n",
    "        text = textwrap.fill(gt_vis_data[i][1][:textlength]+'...', width=20)\n",
    "        text_box = TextArea(text, textprops=dict(color='black', size=7, ha='center', va='bottom'))\n",
    "        ab = AnnotationBbox(text_box, (x, y), frameon=True, bboxprops=dict(boxstyle='round,pad=0.5', facecolor='white', edgecolor='black', alpha=0.7))\n",
    "        ax.add_artist(ab)\n",
    "    else:\n",
    "        img = denormalize(gt_vis_data[i][0]).permute(1, 2, 0)\n",
    "        ax.add_artist(AnnotationBbox(OffsetImage(img, zoom=0.35), (x, y), frameon=True))\n",
    "    # plot a blue circle around the gt with legend\n",
    "    if i == 0:\n",
    "        ax.scatter(x, y, s=100, facecolors='none', edgecolors='green', label='Ground Truth')\n",
    "    ax.scatter(x, y, s=radius, facecolors='none', edgecolors='green')\n",
    "\n",
    "# PLot query\n",
    "for i, (x, y) in enumerate(queries_2d):\n",
    "    if config['mode'] == \"img2text\":\n",
    "        img = denormalize(gt_vis_data[i][0]).permute(1, 2, 0)\n",
    "        ax.add_artist(AnnotationBbox(OffsetImage(img, zoom=0.35), (x, y), frameon=True))\n",
    "    else:\n",
    "        text = textwrap.fill(gt_vis_data[i][1][:textlength]+'...', width=20)\n",
    "        text_box = TextArea(text, textprops=dict(color='black', size=7, ha='center', va='bottom'))\n",
    "        ab = AnnotationBbox(text_box, (x, y), frameon=True, bboxprops=dict(boxstyle='round,pad=0.5', facecolor='white', edgecolor='black', alpha=0.7))\n",
    "        ax.add_artist(ab)\n",
    "\n",
    "    offset = 15\n",
    "    zoom = 0.05 if config['mode'] == \"img2text\" else 0.04\n",
    "    if predictions[i][0] == labels[i]:\n",
    "        #plot a green tick emoji in the right bottom corner\n",
    "        img = plt.imread(imojify.get_img_path('✅'))\n",
    "        ax.add_artist(AnnotationBbox(OffsetImage(img, zoom=zoom), (x+offset, y-offset), frameon=False))\n",
    "    else:\n",
    "        #plot a red cross emoji in the right bottom corner\n",
    "        img = plt.imread(imojify.get_img_path('❌'))\n",
    "        ax.add_artist(AnnotationBbox(OffsetImage(img, zoom=zoom), (x+offset, y-offset), frameon=False))\n",
    "    \n",
    "    # plot a blue circle around the query with legend\n",
    "    if i == 0:\n",
    "        ax.scatter(x, y, s=100, facecolors='none', edgecolors='blue', label='Query')\n",
    "    ax.scatter(x, y, s=15000 if config['mode'] == \"img2text\" else 5000, facecolors='none', edgecolors='blue')\n",
    "\n",
    "xmin = X_embedded[:, 0].min()\n",
    "xmax = X_embedded[:, 0].max()\n",
    "ymin = X_embedded[:, 1].min()\n",
    "ymax = X_embedded[:, 1].max()\n",
    "ax.set_xlim(xmin - abs(0.4 * xmin), xmax + abs(0.4 * xmax))\n",
    "ax.set_ylim(ymin - abs(0.4 * ymin), ymax + abs(0.4 * ymax))\n",
    "# ax.set_xlim(25,  xmax + abs(0.4 * xmax))\n",
    "# ax.set_ylim(-150, 50)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
