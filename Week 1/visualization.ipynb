{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from data_utils import MIT_split_dataset, CustomTransform\n",
    "from torch.utils.data import DataLoader\n",
    "from model_luis import Model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, confusion_matrix, ConfusionMatrixDisplay, PrecisionRecallDisplay\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of a set of predictions compared to the actual labels.\n",
    "\n",
    "    Parameters:\n",
    "        predictions: numpy array containing the predicted values.\n",
    "        labels: numpy array containing the actual labels.\n",
    "\n",
    "    Returns:\n",
    "        A float representing the accuracy.\n",
    "    \"\"\"\n",
    "    return sum(predictions == labels) / len(labels)\n",
    "\n",
    "def precision(predictions, labels, class_label):\n",
    "    \"\"\"\n",
    "    Calculates precision for a specific class in a classification task.\n",
    "\n",
    "    Parameters:\n",
    "        predictions: numpy array containing the predicted class labels.\n",
    "        labels: numpy array containing the actual class labels.\n",
    "        class_label: the specific class for which precision is calculated.\n",
    "\n",
    "    Returns:\n",
    "        Precision value for the specified class.\n",
    "    \"\"\"\n",
    "    tp = np.sum((predictions == class_label) & (labels == class_label))\n",
    "    fp = np.sum((predictions == class_label) & (labels != class_label))\n",
    "    return tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "def recall(predictions, labels, class_label):\n",
    "    \"\"\"\n",
    "    Calculates recall for a specific class in a classification task.\n",
    "\n",
    "    Parameters:\n",
    "        predictions: numpy array containing the predicted class labels.\n",
    "        labels: numpy array containing the actual class labels.\n",
    "        class_label: the specific class for which recall is calculated.\n",
    "\n",
    "    Returns:\n",
    "        Recall value for the specified class.\n",
    "    \"\"\"\n",
    "    tp = np.sum((predictions == class_label) & (labels == class_label))\n",
    "    fn = np.sum((predictions != class_label) & (labels == class_label))\n",
    "    return tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "def average_precision(predictions, labels):\n",
    "    \"\"\"\n",
    "    Calculates the average precision across all classes in a classification task.\n",
    "\n",
    "    Parameters:\n",
    "        predictions: numpy array containing the predicted class labels.\n",
    "        labels: numpy array containing the actual class labels.\n",
    "\n",
    "    Returns:\n",
    "        The average precision across all classes.\n",
    "    \"\"\"\n",
    "    classes = np.unique(labels)\n",
    "    return np.mean([precision(predictions, labels, c) for c in classes])\n",
    "\n",
    "def average_recall(predictions, labels):\n",
    "    \"\"\"\n",
    "    Calculates the average recall across all classes in a classification task.\n",
    "\n",
    "    Parameters:\n",
    "        predictions: numpy array containing the predicted class labels.\n",
    "        labels: numpy array containing the actual class labels.\n",
    "\n",
    "    Returns:\n",
    "        The average recall across all classes.\n",
    "    \"\"\"\n",
    "    classes = np.unique(labels)\n",
    "    return np.mean([recall(predictions, labels, c) for c in classes])\n",
    "\n",
    "def average_f1(predictions, labels):\n",
    "    \"\"\"\n",
    "    Calculates the average F1 score across all classes in a classification task.\n",
    "\n",
    "    Parameters:\n",
    "        predictions: numpy array containing the predicted class labels.\n",
    "        labels: numpy array containing the actual class labels.\n",
    "\n",
    "    Returns:\n",
    "        The average F1 score across all classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    return 2 * average_precision(predictions, labels) * average_recall(predictions, labels) / (average_precision(predictions, labels) + average_recall(predictions, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'IMG_WIDTH': 256,\n",
    "    'IMG_HEIGHT': 256,\n",
    "    'TEST_DATASET_DIR': 'data/MIT_split/test',\n",
    "    'batch_size': 32\n",
    "}\n",
    "\n",
    "transform_test = CustomTransform(config, mode='test')\n",
    "dataset_test = MIT_split_dataset(config['TEST_DATASET_DIR'], transform=transform_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "model = Model(num_classes=8)\n",
    "model.load_state_dict(torch.load('pretrained/best_model.pth'))\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    labels_pred = []\n",
    "    labels_true = []\n",
    "    predictions = []\n",
    "    for inputs, labels in dataloader_test:\n",
    "        outputs = F.softmax(model(inputs.cuda()), dim=1)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        labels_pred.extend(preds.cpu().numpy())\n",
    "        labels_true.extend(labels.numpy())\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "labels_pred = np.array(labels_pred)\n",
    "labels_true = np.array(labels_true)\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "cm = confusion_matrix(labels_true, labels_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['coast', 'forest', 'highway', 'inside_city', 'mountain', 'Opencountry', 'street', 'tallbuilding'])\n",
    "_, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(ax=ax, cmap=plt.cm.Blues, xticks_rotation='vertical', colorbar=False)\n",
    "\n",
    "# Adding text for metrics\n",
    "textstr = '\\n'.join((\n",
    "    f'Accuracy: {accuracy(labels_pred, labels_true):.3f}',\n",
    "    f'Avg. Precision: {average_precision(labels_pred, labels_true):.3f}',\n",
    "    f'Avg. Recall: {average_recall(labels_pred, labels_true):.3f}',\n",
    "    f'Avg. F1-score: {average_f1(labels_pred, labels_true):.3f}'))\n",
    "\n",
    "# These are matplotlib.patch.Patch properties\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "# Place a text box in upper left in axes coords\n",
    "ax.text(1.03, 0.55, textstr, transform=ax.transAxes, fontsize=12,\n",
    "        verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels to binary format for each class\n",
    "n_classes = 8  # Number of classes\n",
    "labels_binary = label_binarize(labels_true, classes=[i for i in range(n_classes)])\n",
    "\n",
    "# Calculate precision and recall for each class\n",
    "precision_dict = dict()\n",
    "recall_dict = dict()\n",
    "average_precision_dict = dict()\n",
    "for i in range(n_classes):\n",
    "    precision_dict[i], recall_dict[i], _ = precision_recall_curve(labels_binary[:, i], predictions[:, i])\n",
    "    average_precision_dict[i] = average_precision_score(labels_binary[:, i], predictions[:, i])\n",
    "\n",
    "# Compute micro-average precision and recall\n",
    "precision_micro, recall_micro, _ = precision_recall_curve(labels_binary.ravel(), predictions.ravel())\n",
    "average_precision_micro = average_precision_score(labels_binary, predictions, average=\"micro\")\n",
    "\n",
    "classes = ['\"coast\"', '\"forest\"', '\"highway\"', '\"inside_city\"', '\"mountain\"', '\"opencountry\"', '\"street\"', '\"tallbuilding\"']\n",
    "\n",
    "# Plot PR curves for each class\n",
    "_, ax = plt.subplots(figsize=(6, 6))\n",
    "colors = [\"tab:red\", \"tab:green\", \"tab:blue\", \"tab:brown\", \"tab:orange\", \"tab:purple\", \"tab:pink\", \"brown\"]\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(recall_dict[i], precision_dict[i], color=color, alpha=0.35,\n",
    "             label=f'PR curve of class {classes[i]} (area = {average_precision_dict[i]:0.2f})')\n",
    "\n",
    "# Plot micro-averaged PR curve\n",
    "display = PrecisionRecallDisplay(\n",
    "    recall=recall_micro,\n",
    "    precision=precision_micro,\n",
    "    average_precision=average_precision_micro,\n",
    "    prevalence_pos_label=Counter(labels_binary.ravel())[1] / labels_binary.size,\n",
    ")\n",
    "display.plot(ax=ax, name=\"Micro-average\", plot_chance_level=True, color='black')\n",
    "\n",
    "# Plot iso F1 curves\n",
    "f1_scores = np.linspace(0.2, 0.8, num=4)\n",
    "for f1_score in f1_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f1_score * x / (2 * x - f1_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate(f'f1={f1_score:0.1f}', xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "# Customizing the plot\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Micro-averaged Precision-Recall curve')\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
