{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "from data_utils import *\n",
    "import fasttext, re, json, faiss\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from model import Net\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'IMG_WIDTH': 224,\n",
    "    'IMG_HEIGHT': 224,\n",
    "    'TEST_DATASET_DIR': 'data/val2014',\n",
    "    'batch_size': 16,\n",
    "    'epochs': 80,\n",
    "    'n_neighbors': 5,\n",
    "    'classifier': 'knn',\n",
    "    'text_encoder_type': 'ft',\n",
    "    'mode': \"img2text\",\n",
    "    'metric': 'euclidean',\n",
    "    'voronoi_cells': 64,\n",
    "    'lookup_cells': 8,\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File containing images info (file_name)\n",
    "with open(f\"/home/mcv/datasets/C5/COCO/captions_val2014.json\", 'r') as f:\n",
    "    captions_val = json.load(f)\n",
    "\n",
    "# Dataset for the retrieval\n",
    "retrieval_dataset = CocoMetricDataset(\n",
    "    root=config[\"TEST_DATASET_DIR\"],\n",
    "    captions_file=captions_val,\n",
    "    transforms=CustomTransform(config, mode=\"train\"))\n",
    "\n",
    "total_length = len(retrieval_dataset)\n",
    "train_size = int(0.6 * total_length)  # e.g., 60% for training\n",
    "valid_size = int(0.2 * total_length)  # e.g., 20% for validation\n",
    "test_size = total_length - train_size - valid_size # remaining 20% for testing\n",
    "train_dataset, validation_dataset, test_dataset = random_split(retrieval_dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=coco_collator)\n",
    "dataloader_validation = DataLoader(validation_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=coco_collator)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=coco_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(config['text_encoder_type'])\n",
    "model.load_state_dict(torch.load(\"pretrained/img2text_hard_ft.pth\", map_location=config['device']))\n",
    "model = model.to(config['device']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(loader, model, device, mode=None, query=False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features, labels = [], []\n",
    "        for img, captions, label in tqdm(loader, total=len(loader), desc='Extracting features'):\n",
    "            if mode == \"img2text\":\n",
    "                if query:\n",
    "                    f, _ = model(img.to(device), captions)\n",
    "                else:\n",
    "                    _, f = model(img.to(device), captions)\n",
    "            else:\n",
    "                if query:\n",
    "                    _, f = model(img.to(device), captions)\n",
    "                else:\n",
    "                    f, _ = model(img.to(device), captions)\n",
    "\n",
    "            features.append(f.cpu().numpy())\n",
    "            labels.append(label)\n",
    "        return np.concatenate(features).astype('float32'), np.concatenate(labels)\n",
    "\n",
    "# k-NN Classifier for Image Retrieval\n",
    "class ImageRetrievalSystem:\n",
    "    def __init__(self, model, database_loader, query_loader, config):\n",
    "        self.model = model\n",
    "        self.database_loader = database_loader\n",
    "        self.query_loader = query_loader\n",
    "        self.device = config['device']\n",
    "        self.mode = config['mode']\n",
    "\n",
    "        self.dim = 2048\n",
    "        self.classifier_type = config['classifier']\n",
    "        self.n_neighbors = config['n_neighbors']\n",
    "\n",
    "        if self.classifier_type == 'knn':\n",
    "            self.classifier = NearestNeighbors(n_neighbors=config['n_neighbors'], metric=config['metric'])\n",
    "        else:\n",
    "            self.classifier = faiss.IndexIVFFlat(faiss.IndexFlatL2(self.dim), self.dim, config['voronoi_cells'])\n",
    "            self.classifier.nprobe = config['lookup_cells']\n",
    "\n",
    "    def fit(self):\n",
    "\n",
    "        features, self.train_labels = extract_features(self.database_loader, self.model, self.device, mode=self.mode)\n",
    "        \n",
    "        print('Fitting the classifier...')\n",
    "        if self.classifier_type == 'knn':\n",
    "            self.classifier.fit(features)\n",
    "        else:\n",
    "            self.classifier.train(features)\n",
    "            self.classifier.add(features)\n",
    "\n",
    "    def retrieve(self):\n",
    "\n",
    "        features, labels = extract_features(self.query_loader, self.model, self.device, mode=self.mode, query=True)\n",
    "        \n",
    "        print('Retrieving images...')\n",
    "        if self.classifier_type == 'knn':\n",
    "            _, predictions = self.classifier.kneighbors(features, return_distance=True)\n",
    "        else:\n",
    "            _, predictions = self.classifier.search(features, self.n_neighbors)\n",
    "        \n",
    "        return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, labels):\n",
    "\n",
    "    # Prec@1\n",
    "    prec_at_1 = np.mean([1 if predictions[i, 0] == labels[i] else 0 for i in range(predictions.shape[0])])\n",
    "\n",
    "    # Prec@5\n",
    "    prec_at_5 = np.mean([np.any([1 if predictions[i, j] == labels[i] else 0 for j in range(config['n_neighbors'])]) for i in range(predictions.shape[0])])\n",
    "\n",
    "    # Initialize list to store average precision for each query\n",
    "    average_precisions = []\n",
    "\n",
    "    # Compute binary relevance arrays and calculate average precision for each query\n",
    "    for i in range(predictions.shape[0]):\n",
    "        # Convert true label into binary format for each prediction\n",
    "        binary_relevance = np.array([1 if label == labels[i] else 0 for label in predictions[i]])\n",
    "        \n",
    "        # Ensure there is at least one positive class to avoid division by zero in AP score calculation\n",
    "        if np.sum(binary_relevance) > 0:\n",
    "            # Compute the average precision for the current query\n",
    "            ap_score = average_precision_score(binary_relevance, np.ones_like(binary_relevance))\n",
    "            average_precisions.append(ap_score)\n",
    "\n",
    "    # Compute mean Average Precision (mAP) by averaging all the AP scores\n",
    "    mean_ap = np.mean(average_precisions)\n",
    "\n",
    "    return prec_at_1, prec_at_5, mean_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ImageRetrievalSystem(model, dataloader_train, dataloader_validation, config)\n",
    "pipeline.fit()\n",
    "predictions, labels = pipeline.retrieve()\n",
    "\n",
    "predictions = pipeline.train_labels[predictions]\n",
    "\n",
    "prec_at_1, prec_at_5, mean_ap = evaluate(predictions, labels)\n",
    "\n",
    "print(f'Precision at 1: {prec_at_1:.4f}')\n",
    "print(f'Precision at 5: {prec_at_5:.4f}')\n",
    "print(f'Mean Average Precision: {mean_ap:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select index of correct/incorrect predictions\n",
    "idx = 0\n",
    "d_db = dict()\n",
    "d_gt = dict()\n",
    "for x in train_dataset:\n",
    "    if x[2] in predictions[idx]:\n",
    "        if x[2] not in d_db:\n",
    "            d_db[x[2]] = [[x[0]], [x[1]]]\n",
    "        else:\n",
    "            d_db[x[2]][0].append(x[0])\n",
    "            d_db[x[2]][1].append(x[1])\n",
    "\n",
    "    if x[2] == labels[idx]:\n",
    "        if x[2] not in d_gt:\n",
    "            d_gt[x[2]] = [[x[0]], [x[1]]]\n",
    "        else:\n",
    "            d_gt[x[2]][0].append(x[0])\n",
    "            d_gt[x[2]][1].append(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image, query_text, _ = validation_dataset[idx]\n",
    "with torch.no_grad():\n",
    "    query_image = query_image.unsqueeze(0).to(config['device'])\n",
    "    query_image_embedding, query_text_embedding = pipeline.model(query_image, [query_text])\n",
    "\n",
    "query_embedding = query_image_embedding if config[\"mode\"] == \"img2text\" else query_text_embedding\n",
    "query_embeddings = [query_embedding.cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_embeddings = []\n",
    "for k, v in d_db.items():\n",
    "    for image, text in zip(v[0], v[1]):\n",
    "        with torch.no_grad():\n",
    "            image = image.unsqueeze(0).to(config['device'])\n",
    "            f_image, f_text = model(image, [text])\n",
    "            f = f_text if config[\"mode\"] == \"img2text\" else f_image\n",
    "            retrieved_embeddings.append(f.cpu().numpy())\n",
    "\n",
    "gt_embeddings = []\n",
    "for k, v in d_gt.items():\n",
    "    for image, text in zip(v[0], v[1]):\n",
    "        with torch.no_grad():\n",
    "            image = image.unsqueeze(0).to(config['device'])\n",
    "            f_image, f_text = model(image, [text])\n",
    "            f = f_text if config[\"mode\"] == \"img2text\" else f_image\n",
    "            gt_embeddings.append(f.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to restore dataset images to original values\n",
    "unnormalize = transforms.Compose([\n",
    "    transforms.Normalize(mean=[ 0., 0., 0. ], std=[ 1/0.229, 1/0.224, 1/0.225]),\n",
    "    transforms.Normalize(mean=[ -0.485, -0.456, -0.406 ], std=[ 1., 1., 1. ])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap.umap_ import UMAP\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox, TextArea\n",
    "import textwrap\n",
    "\n",
    "# Combine both sets of embeddings for UMAP\n",
    "query_embeddings, retrieved_embeddings, gt_embeddings = np.unique(query_embeddings, axis=0), np.unique(retrieved_embeddings, axis=0), np.unique(gt_embeddings, axis=0)\n",
    "combined_embeddings = np.vstack([query_embeddings, retrieved_embeddings, gt_embeddings]).reshape(-1, 2048)\n",
    "\n",
    "# Perform UMAP\n",
    "umap = UMAP(n_neighbors=5, min_dist=0.0, metric='euclidean', random_state=123, n_jobs=1)\n",
    "umap_result = umap.fit_transform(combined_embeddings)\n",
    "\n",
    "# Split the transformed embeddings back into queries and images\n",
    "queries_2d, retrieved_2d, gt_2d = umap_result[:len(query_embeddings)], umap_result[len(query_embeddings):len(query_embeddings)+len(retrieved_embeddings)], umap_result[len(query_embeddings)+len(retrieved_embeddings):]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot retrieved\n",
    "values = np.unique([xi for x in list(d_db.values()) for xi in x[0]], axis=0)\n",
    "for i, (x, y) in enumerate(retrieved_2d):\n",
    "    img = np.moveaxis(unnormalize(torch.tensor(values[i])).numpy(), 0, 2)\n",
    "    imgbox = OffsetImage(img, zoom=0.2)\n",
    "    ab = AnnotationBbox(imgbox, (x, y), frameon=True)\n",
    "    ax.add_artist(ab)\n",
    "    # plot a red circle around the retrieved with legend\n",
    "    if i == 0:\n",
    "        ax.scatter(x, y, s=100, facecolors='none', edgecolors='red', label='Retrieved')\n",
    "    ax.scatter(x, y, s=5000, facecolors='none', edgecolors='red')\n",
    "\n",
    "# Plot gt\n",
    "values = np.unique([xi for x in list(d_gt.values()) for xi in x[0]], axis=0)\n",
    "for i, (x, y) in enumerate(gt_2d):\n",
    "    img = np.moveaxis(unnormalize(torch.tensor(values[i])).numpy(), 0, 2)\n",
    "    imgbox = OffsetImage(img, zoom=0.2)\n",
    "    ab = AnnotationBbox(imgbox, (x, y), frameon=True)\n",
    "    ax.add_artist(ab)\n",
    "    # plot a green circle around the ground truth with legend\n",
    "    if i == 0:\n",
    "        ax.scatter(x, y, s=100, facecolors='none', edgecolors='green', label='Ground Truth')\n",
    "    ax.scatter(x, y, s=5000, facecolors='none', edgecolors='green')\n",
    "\n",
    "# PLot query\n",
    "for i, (x, y) in enumerate(queries_2d):\n",
    "    text = textwrap.fill(query_text, width=20)\n",
    "    text_box = TextArea(text, textprops=dict(color='black', size=10, ha='center', va='bottom'))\n",
    "    ab = AnnotationBbox(text_box, (x, y), frameon=True, bboxprops=dict(boxstyle='round,pad=0.5', facecolor='white', edgecolor='black', alpha=0.7))\n",
    "    ax.add_artist(ab)\n",
    "    \n",
    "ax.set_xlim(umap_result[:, 0].min() - 1, umap_result[:, 0].max() + 1)\n",
    "ax.set_ylim(umap_result[:, 1].min() - 1, umap_result[:, 1].max() + 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
