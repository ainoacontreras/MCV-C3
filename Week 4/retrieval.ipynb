{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from tqdm import tqdm\n",
    "from data_utils import *\n",
    "import fasttext, re, json, faiss\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from model import Net\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'IMG_WIDTH': 224,\n",
    "    'IMG_HEIGHT': 224,\n",
    "    'TEST_DATASET_DIR': 'data/val2014',\n",
    "    'batch_size': 16,\n",
    "    'epochs': 80,\n",
    "    'n_neighbors': 5,\n",
    "    'classifier': 'knn',\n",
    "    'text_encoder_type': 'ft',\n",
    "    'mode': \"img2text\",\n",
    "    'metric': 'euclidean',\n",
    "    'voronoi_cells': 64,\n",
    "    'lookup_cells': 8,\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating image-answer pairs...: 100%|██████████| 5000/5000 [00:13<00:00, 370.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# File containing images info (file_name)\n",
    "with open(f\"data/captions_val2014.json\", 'r') as f:\n",
    "    captions_val = json.load(f)\n",
    "\n",
    "# Dataset for the retrieval\n",
    "retrieval_dataset = CocoMetricDataset(\n",
    "    root=config[\"TEST_DATASET_DIR\"],\n",
    "    captions_file=captions_val,\n",
    "    transforms=CustomTransform(config, mode=\"train\"))\n",
    "\n",
    "dataloader = DataLoader(retrieval_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=coco_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Luis/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model = Net(config['text_encoder_type'])\n",
    "# model.load_state_dict(torch.load(\"pretrained/img2text_hard_ft.pth\", map_location=config['device']))\n",
    "model = model.to(config['device']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(loader, model, device, mode=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        db_features, query_features, labels = [], [], []\n",
    "        for img, captions, label in tqdm(loader, total=len(loader), desc='Extracting features'):\n",
    "            img_f, text_f = model(img.to(device), captions)\n",
    "            if mode == \"img2text\":\n",
    "                db_features.append(text_f.cpu().numpy())\n",
    "                query_features.append(img_f.cpu().numpy())\n",
    "            else:\n",
    "                db_features.append(img_f.cpu().numpy())\n",
    "                query_features.append(text_f.cpu().numpy())\n",
    "\n",
    "            labels.append(label)\n",
    "        return np.concatenate(db_features).astype('float32'), np.concatenate(query_features).astype('float32'), np.concatenate(labels)\n",
    "\n",
    "# k-NN Classifier for Image Retrieval\n",
    "class ImageRetrievalSystem:\n",
    "    def __init__(self, model, database_loader, config):\n",
    "        self.model = model\n",
    "        self.database_loader = database_loader\n",
    "        self.device = config['device']\n",
    "        self.mode = config['mode']\n",
    "\n",
    "        self.dim = 2048\n",
    "        self.classifier_type = config['classifier']\n",
    "        self.n_neighbors = config['n_neighbors']\n",
    "\n",
    "        if self.classifier_type == 'knn':\n",
    "            self.classifier = NearestNeighbors(n_neighbors=config['n_neighbors'], metric=config['metric'])\n",
    "        else:\n",
    "            self.classifier = faiss.IndexIVFFlat(faiss.IndexFlatL2(self.dim), self.dim, config['voronoi_cells'])\n",
    "            self.classifier.nprobe = config['lookup_cells']\n",
    "\n",
    "    def fit_and_retrieve(self):\n",
    "\n",
    "        db_features, query_features, labels = extract_features(self.database_loader, self.model, self.device, mode=self.mode)\n",
    "        \n",
    "        if self.classifier_type == 'knn':\n",
    "            self.classifier.fit(db_features)\n",
    "            _, predictions = self.classifier.kneighbors(query_features, return_distance=True)\n",
    "        else:\n",
    "            self.classifier.train(db_features)\n",
    "            self.classifier.add(db_features)\n",
    "            _, predictions = self.classifier.search(query_features, self.n_neighbors)\n",
    "\n",
    "        return predictions, labels   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, labels):\n",
    "\n",
    "    # Prec@1\n",
    "    prec_at_1 = np.mean([1 if predictions[i, 0] == labels[i] else 0 for i in range(predictions.shape[0])])\n",
    "\n",
    "    # Prec@5\n",
    "    prec_at_5 = np.mean([np.any([1 if predictions[i, j] == labels[i] else 0 for j in range(config['n_neighbors'])]) for i in range(predictions.shape[0])])\n",
    "\n",
    "    # Initialize list to store average precision for each query\n",
    "    average_precisions = []\n",
    "\n",
    "    # Compute binary relevance arrays and calculate average precision for each query\n",
    "    for i in range(predictions.shape[0]):\n",
    "        # Convert true label into binary format for each prediction\n",
    "        binary_relevance = np.array([1 if label == labels[i] else 0 for label in predictions[i]])\n",
    "        \n",
    "        # Ensure there is at least one positive class to avoid division by zero in AP score calculation\n",
    "        if np.sum(binary_relevance) > 0:\n",
    "            # Compute the average precision for the current query\n",
    "            ap_score = average_precision_score(binary_relevance, np.ones_like(binary_relevance))\n",
    "            average_precisions.append(ap_score)\n",
    "\n",
    "    # Compute mean Average Precision (mAP) by averaging all the AP scores\n",
    "    mean_ap = np.mean(average_precisions)\n",
    "\n",
    "    return prec_at_1, prec_at_5, mean_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 188/188 [11:23<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 63/63 [02:59<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving images...\n",
      "Precision at 1: 0.0000\n",
      "Precision at 5: 0.0000\n",
      "Mean Average Precision: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luis\\miniconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Luis\\miniconda3\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "pipeline = ImageRetrievalSystem(model, dataloader, config)\n",
    "predictions_idx, labels = pipeline.fit_and_retrieve()\n",
    "\n",
    "predictions = labels[predictions_idx]\n",
    "\n",
    "prec_at_1, prec_at_5, mean_ap = evaluate(predictions, labels)\n",
    "\n",
    "print(f'Precision at 1: {prec_at_1:.4f}')\n",
    "print(f'Precision at 5: {prec_at_5:.4f}')\n",
    "print(f'Mean Average Precision: {mean_ap:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to denormalize image\n",
    "def denormalize(normalized_img):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    denormalized_img = normalized_img * std[:, None, None] + mean[:, None, None]\n",
    "    return denormalized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "retrieval_vis_data = [retrieval_dataset[x] for x in np.unique(predictions_idx[:n].flatten())]\n",
    "gt_vis_data = [retrieval_dataset[x] for x in range(n)]\n",
    "\n",
    "retrieved_embeddings = []\n",
    "for img, text, _ in retrieval_vis_data:\n",
    "    with torch.no_grad():\n",
    "        img_f, text_f = model(img.unsqueeze(0).to(config['device']), [text])\n",
    "    if config['mode'] == \"img2text\":\n",
    "        retrieved_embeddings.append(text_f.cpu().numpy())\n",
    "    else:\n",
    "        retrieved_embeddings.append(img_f.cpu().numpy())\n",
    "\n",
    "gt_embeddings, query_embeddings = [], []\n",
    "for img, text, _ in gt_vis_data:\n",
    "    with torch.no_grad():\n",
    "        img_f, text_f = model(img.unsqueeze(0).to(config['device']), [text])\n",
    "    if config['mode'] == \"img2text\":\n",
    "        gt_embeddings.append(text_f.cpu().numpy())\n",
    "        query_embeddings.append(img_f.cpu().numpy())\n",
    "    else:\n",
    "        gt_embeddings.append(img_f.cpu().numpy())\n",
    "        query_embeddings.append(text_f.cpu().numpy())\n",
    "\n",
    "retrieved_embeddings = np.concatenate(retrieved_embeddings).astype('float32')\n",
    "query_embeddings = np.concatenate(query_embeddings).astype('float32')\n",
    "gt_embeddings = np.concatenate(gt_embeddings).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox, TextArea\n",
    "import textwrap\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.lines as mlines\n",
    "from imojify import imojify\n",
    "\n",
    "textlength = 50\n",
    "\n",
    "# Combine both sets of embeddings for UMAP\n",
    "combined_embeddings = np.vstack([query_embeddings, retrieved_embeddings, gt_embeddings]).reshape(-1, pipeline.dim)\n",
    "\n",
    "# Perform t-SNE on the combined embeddings\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto', perplexity=0.001, metric=config['metric'], random_state=123).fit_transform(combined_embeddings)\n",
    "\n",
    "# Split the transformed embeddings back into queries and images\n",
    "queries_2d, retrieved_2d, gt_2d = X_embedded[:len(query_embeddings)], X_embedded[len(query_embeddings):len(query_embeddings)+len(retrieved_embeddings)], X_embedded[len(query_embeddings)+len(retrieved_embeddings):]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot retrieved\n",
    "for i, (x, y) in enumerate(retrieved_2d):\n",
    "    if config['mode'] == \"img2text\": # retrieve text\n",
    "        text = textwrap.fill(retrieval_vis_data[i][1][:textlength]+'...', width=20)\n",
    "        text_box = TextArea(text, textprops=dict(color='black', size=7, ha='center', va='bottom'))\n",
    "        ab = AnnotationBbox(text_box, (x, y), frameon=True, bboxprops=dict(boxstyle='round,pad=0.5', facecolor='white', edgecolor='black', alpha=0.7))\n",
    "        ax.add_artist(ab)\n",
    "    else:\n",
    "        img = denormalize(retrieval_vis_data[i][0]).permute(1, 2, 0)\n",
    "        imgbox = OffsetImage(img, zoom=0.35)\n",
    "        ab = AnnotationBbox(imgbox, (x, y), frameon=True)\n",
    "        ax.add_artist(ab)\n",
    "    # plot a red circle around the retrieved with legend\n",
    "    if i == 0:\n",
    "        ax.scatter(x, y, s=100, facecolors='none', edgecolors='red', label='Retrieved')\n",
    "    ax.scatter(x, y, s=5000, facecolors='none', edgecolors='red')\n",
    "\n",
    "# Plot gt\n",
    "for i, (x, y) in enumerate(gt_2d):\n",
    "    if config['mode'] == \"img2text\": # retrieve text\n",
    "        text = textwrap.fill(gt_vis_data[i][1][:textlength]+'...', width=20)\n",
    "        text_box = TextArea(text, textprops=dict(color='black', size=7, ha='center', va='bottom'))\n",
    "        ab = AnnotationBbox(text_box, (x, y), frameon=True, bboxprops=dict(boxstyle='round,pad=0.5', facecolor='white', edgecolor='black', alpha=0.7))\n",
    "        ax.add_artist(ab)\n",
    "    else:\n",
    "        img = denormalize(gt_vis_data[i][0]).permute(1, 2, 0)\n",
    "        ax.add_artist(AnnotationBbox(OffsetImage(img, zoom=0.35), (x, y), frameon=True))\n",
    "    # plot a blue circle around the gt with legend\n",
    "    if i == 0:\n",
    "        ax.scatter(x, y, s=100, facecolors='none', edgecolors='green', label='Ground Truth')\n",
    "    ax.scatter(x, y, s=5000, facecolors='none', edgecolors='green')\n",
    "\n",
    "# PLot query\n",
    "for i, (x, y) in enumerate(queries_2d):\n",
    "    if config['mode'] == \"img2text\":\n",
    "        img = denormalize(gt_vis_data[i][0]).permute(1, 2, 0)\n",
    "        ax.add_artist(AnnotationBbox(OffsetImage(img, zoom=0.35), (x, y), frameon=True))\n",
    "        if predictions[i][0] == labels[i]:\n",
    "            #plot a green tick emoji in the right bottom corner\n",
    "            img = plt.imread(imojify.get_img_path('✅'))\n",
    "            ax.add_artist(AnnotationBbox(OffsetImage(img, zoom=0.05), (x+200, y-350), frameon=False))\n",
    "    else:\n",
    "        text = textwrap.fill(gt_vis_data[i][1][:textlength]+'...', width=20)\n",
    "        text_box = TextArea(text, textprops=dict(color='black', size=7, ha='center', va='bottom'))\n",
    "        ab = AnnotationBbox(text_box, (x, y), frameon=True, bboxprops=dict(boxstyle='round,pad=0.5', facecolor='white', edgecolor='black', alpha=0.7))\n",
    "        ax.add_artist(ab)\n",
    "    \n",
    "    # plot a blue circle around the query with legend\n",
    "    if i == 0:\n",
    "        ax.scatter(x, y, s=100, facecolors='none', edgecolors='blue', label='Query')\n",
    "    ax.scatter(x, y, s=15000, facecolors='none', edgecolors='blue')\n",
    "\n",
    "xmin = X_embedded[:, 0].min()\n",
    "xmax = X_embedded[:, 0].max()\n",
    "ymin = X_embedded[:, 1].min()\n",
    "ymax = X_embedded[:, 1].max()\n",
    "ax.set_xlim(xmin - abs(0.4 * xmin), xmax + abs(0.4 * xmax))\n",
    "ax.set_ylim(ymin - abs(0.4 * ymin), ymax + abs(0.4 * ymax))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
